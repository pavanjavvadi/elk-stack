input {
    jdbc {
        jdbc_driver_library => "/usr/share/logstash/logstash-core/lib/jars/postgresql.jar"
        jdbc_driver_class => "org.postgresql.Driver"
        jdbc_connection_string => "jdbc:postgresql://ec2-107-22-245-82.compute-1.amazonaws.com:5432/d3jvud32sm2sd8?ssl=false"
        jdbc_user => "fzevdofjgtnjoo"
        jdbc_password => "7c0c05b92f37fc61b8eed7884222ede3ae50dc3ed5f0c3b98c81f8422466125a"
        jdbc_validate_connection => true
        schedule => "* * * * *"
        statement => "SELECT shipment.id, shipment.created_at, shipment.updated_at, business_id_id, business_carrier_id_id, extra_fields::text as extra_fields, customer.*, tracking_number, order_id, shipment_type, status, pickup_date, current_status_description, pickup_from, deliver_to, shipment_time, is_active, has_changed FROM shipments_shipment as shipment, shipments_customer as customer WHERE shipment.customer_id=customer.id AND shipment.id > :sql_last_value;"
        use_column_value => true
        tracking_column_type => "numeric"
        tracking_column => "id"
        last_run_metadata_path => "/usr/share/logstash/.logstash_jdbc_shipments_last_run"
    }
}

filter {
    ruby {
        code => "
            require 'json'
            begin
                data_json = JSON.parse(event.get('extra_fields').to_s || {})
                event.set('extra_fields', data_json)
            rescue Exception => e
                event.tag('invalide extra_fields json')
            end
        "
    }
}

filter {
    mutate {
        remove_field => ["@version", "@timestamp", "tags"]
    }
}


output {
    elasticsearch {
        manage_template => true
        template => "/usr/share/logstash/config/logstash_templates/shipments.json"
        template_name => "shipments"
        template_overwrite => true
        hosts => ["http://elasticsearch:9200"]
        index => "shipments"
        document_id => "%{order_id}"
        doc_as_upsert => true        
    }
}
